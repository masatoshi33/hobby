{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# プロダクト開発演習\n",
    "# mnistを用いて、訓練データが少ない時それらを回転したデータで水増しをすることによる精度の向上がどの程度か確かめる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage, misc\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "# 乱数シードを指定\n",
    "np.random.seed(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 784)\n",
      "(640, 784)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if os.path.exists('mnist_784'):\n",
    "    with open('mnist_784','rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "else:\n",
    "    mnist = datasets.fetch_openml('mnist_784')\n",
    "    with open('mnist_784', 'wb') as f:\n",
    "        pickle.dump(mnist, f)\n",
    "# 訓練データ、テストデータを取ってくる\n",
    "X, T = mnist.data, mnist.target\n",
    "# 訓練データとテストデータに分ける\n",
    "x_train, x_test, t_train, t_test = train_test_split(X, T, test_size=0.2)\n",
    "# データの形を確認\n",
    "print(x_train.shape)\n",
    "#print(t_train.shape)\n",
    "# 640個だけランダムにデータを抽出\n",
    "x_train2 = np.zeros((640,784))\n",
    "t_train2 = np.zeros(640)\n",
    "perm = np.random.permutation(len(x_train))\n",
    "for idx in range(640):\n",
    "    x_train2[idx] = x_train[perm[idx]]\n",
    "    t_train2[idx] = t_train[perm[idx]]\n",
    "# int型のone-hot-vectorに変換\n",
    "t_train = np.eye(10)[t_train.astype(\"int\")]\n",
    "t_train2 = np.eye(10)[t_train2.astype(\"int\")]\n",
    "t_test = np.eye(10)[t_test.astype(\"int\")]\n",
    "# 訓練データの形を確認\n",
    "print(x_train2.shape)\n",
    "#print(t_train2.shape)\n",
    "#print(x_train2[0])\n",
    "#noise = np.random.randint(0, 783 , 30)\n",
    "#x_train2[0][noise] = 255.\n",
    "# x_train2にノイズを加える\n",
    "#for i in range(640):\n",
    "#    noise = np.random.randint(0, 783 , 30) #0から783までの乱数を30個作る\n",
    "#    x_train2[i][noise] = 255. #白\n",
    "#    #x_train[i][noise] = (0,0,0) #黒\n",
    "#for i in range(10): #初めの10個で確認\n",
    "#    plt.imshow(x_train2[i].reshape((28,28)))\n",
    "#    plt.show()\n",
    "# x_train2を回転させる\n",
    "print(t_train2[0])\n",
    "x_train3 = x_train2[0].reshape((28, 28))\n",
    "plt.imshow(x_train3)\n",
    "plt.show()\n",
    "img_30 = ndimage.rotate(x_train3, 30, reshape=False)\n",
    "img_330 = ndimage.rotate(x_train3, 330, reshape=False)\n",
    "#確認用\n",
    "#plt.imshow(img_30)\n",
    "#plt.show()\n",
    "#plt.imshow(img_330)\n",
    "#plt.show()\n",
    "#30°回転\n",
    "x_train30_0 = x_train2.reshape((640, 28, 28))\n",
    "x_train30 = np.zeros((640, 784))\n",
    "for i in range(640):\n",
    "    x_train30_0[i] = ndimage.rotate(x_train30_0[i], 30, reshape=False)\n",
    "    x_train30[i] = x_train30_0[i].reshape((784))\n",
    "#plt.imshow(x_train30[1].reshape((28,28)))\n",
    "#plt.show()\n",
    "#330°回転\n",
    "x_train330_0 = x_train2.reshape((640, 28, 28))\n",
    "x_train330 = np.zeros((640, 784))\n",
    "for i in range(640):\n",
    "    x_train330_0[i] = ndimage.rotate(x_train330_0[i], 330, reshape=False)\n",
    "    x_train330[i] = x_train330_0[i].reshape((784))\n",
    "#plt.imshow(x_train330[1].reshape((28,28)))\n",
    "#plt.show()\n",
    "#15°回転\n",
    "x_train15_0 = x_train2.reshape((640, 28, 28))\n",
    "x_train15 = np.zeros((640, 784))\n",
    "for i in range(640):\n",
    "    x_train15_0[i] = ndimage.rotate(x_train15_0[i], 30, reshape=False)\n",
    "    x_train15[i] = x_train15_0[i].reshape((784))\n",
    "#plt.imshow(x_train15[5].reshape((28,28)))\n",
    "#plt.show()\n",
    "#345°回転\n",
    "x_train345_0 = x_train2.reshape((640, 28, 28))\n",
    "x_train345 = np.zeros((640, 784))\n",
    "for i in range(640):\n",
    "    x_train345_0[i] = ndimage.rotate(x_train345_0[i], 30, reshape=False)\n",
    "    x_train345[i] = x_train345_0[i].reshape((784))\n",
    "#plt.imshow(x_train345[5].reshape((28,28)))\n",
    "#plt.show()\n",
    "#10°回転\n",
    "x_train10_0 = x_train2.reshape((640, 28, 28))\n",
    "x_train10 = np.zeros((640, 784))\n",
    "for i in range(640):\n",
    "    x_train10_0[i] = ndimage.rotate(x_train10_0[i], 10, reshape=False)\n",
    "    x_train10[i] = x_train10_0[i].reshape((784))\n",
    "#plt.imshow(x_train10[5].reshape((28,28)))\n",
    "#plt.show()\n",
    "#350°回転\n",
    "x_train350_0 = x_train2.reshape((640, 28, 28))\n",
    "x_train350 = np.zeros((640, 784))\n",
    "for i in range(640):\n",
    "    x_train350_0[i] = ndimage.rotate(x_train350_0[i], 350, reshape=False)\n",
    "    x_train350[i] = x_train350_0[i].reshape((784))\n",
    "#plt.imshow(x_train350[5].reshape((28,28)))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]\n",
    "\n",
    "class Adam:\n",
    "\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]\n",
    "            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * grads[key]**2\n",
    "            m_unbias = self.m[key] / (1-self.beta1 ** self.iter)\n",
    "            v_unbias = self.v[key] / (1-self.beta2 ** self.iter)\n",
    "            params[key] -= self.lr * m_unbias / (np.sqrt(v_unbias) + 1e-7)\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "def softmax(x):\n",
    "    x = x.T\n",
    "    _x = x - np.max(x, axis=0)\n",
    "    _x = np.exp(_x) / np.sum(np.exp(_x), axis=0)\n",
    "    return _x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mnistMultiLayerNet:\n",
    "    \"\"\"\n",
    "    layer0: 784 次元の入力\n",
    "    ↓ w1, b1 で線形結合\n",
    "    ↓ relu で活性化\n",
    "    layer1: 100 次元の隠れ層\n",
    "    ↓ w2, b2 で線形結合\n",
    "    ↓ relu で活性化\n",
    "    layer2: 100 次元の隠れ層\n",
    "    ↓ w3, b3 で線形結合\n",
    "    ↓ relu で活性化\n",
    "    layer3: 100 次元の隠れ層\n",
    "    ↓ w4, b4 で線形結合\n",
    "    ↓ relu で活性化\n",
    "    layer4: 100 次元の隠れ層\n",
    "    ↓ w5, b5 で線形結合\n",
    "    layer5: 10 次元の出力層\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.input_size = 784\n",
    "        self.output_size = 10\n",
    "        self.hidden_size_list = [100, 100, 100, 100]\n",
    "        self.all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
    "        self.hidden_layer_num = len(self.hidden_size_list)\n",
    "        self.weight_decay_lambda =0\n",
    "        self.params = {}\n",
    "        self.layers = {}\n",
    "        self.grads = {}\n",
    "\n",
    "        # 重みとバイアスの初期化\n",
    "        for idx in range(1, len(self.all_size_list)):\n",
    "            self.params['w' + str(idx)] = np.random.randn(self.all_size_list[idx-1], self.all_size_list[idx]) * 0.085\n",
    "            self.params['b' + str(idx)] = np.zeros(self.all_size_list[idx], dtype=float)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        relu = lambda x : np.maximum(0, x)  # 活性化関数として ReLU を使用\n",
    "        self.layers['layer0'] = x\n",
    "        for idx in range(1, len(self.all_size_list) - 1):\n",
    "            w = self.params['w' + str(idx)]\n",
    "            b = self.params['b' + str(idx)]\n",
    "            x = self.layers['layer' + str(idx - 1)]\n",
    "            self.layers['layer' + str(idx)] = relu(np.dot(x, w) + b)\n",
    "        idx = len(self.all_size_list) - 1\n",
    "        w = self.params['w' + str(idx)]\n",
    "        b = self.params['b' + str(idx)]\n",
    "        x = self.layers['layer' + str(idx - 1)]\n",
    "        self.layers['layer' + str(idx)] = softmax(np.dot(x, w) + b)\n",
    "        \n",
    "        return self.layers['layer' + str(idx)]\n",
    "        \n",
    "\n",
    "    def loss(self, y, t):\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def backward(self, t, y):\n",
    "        delta = (y - t) / t.shape[0]\n",
    "        self.grads['b5'] = np.sum(delta, axis=0)\n",
    "        self.grads['w5'] = np.dot(self.layers['layer4'].transpose(), delta)\n",
    "        # 誤差逆伝播\n",
    "        for idx in range(4, 0, -1):\n",
    "            delta = np.dot(delta, self.params['w' + str(idx + 1)].transpose())\n",
    "            delta = delta *  (self.layers['layer' + str(idx)] > 0)\n",
    "            self.grads['b' + str(idx)] = np.sum(delta, axis=0)\n",
    "            self.grads['w' + str(idx)] = np.dot(self.layers['layer'+str(idx - 1)].transpose(), delta)\n",
    "        return self.grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mnistMultiLayerBatchNet:\n",
    "    \"\"\"\n",
    "    layer0: 784 次元の入力\n",
    "    ↓ w1, b1 で線形結合\n",
    "    ↓バッチ正規化 gamma1倍しbeta1だけずらす\n",
    "    ↓ relu で活性化\n",
    "    layer1: 100 次元の隠れ層\n",
    "    ↓ w2, b2 で線形結合\n",
    "    ↓バッチ正規化 gamma2倍しbeta2だけずらす\n",
    "    ↓ relu で活性化\n",
    "    layer2: 100 次元の隠れ層\n",
    "    ↓ w3, b3 で線形結合\n",
    "    ↓バッチ正規化 gamma3倍しbeta3だけずらす\n",
    "    ↓ relu で活性化\n",
    "    layer3: 100 次元の隠れ層\n",
    "    ↓ w4, b4 で線形結合\n",
    "    ↓バッチ正規化 gamma4倍しbeta4だけずらす\n",
    "    ↓ relu で活性化\n",
    "    layer4: 100 次元の隠れ層\n",
    "    ↓ w5, b5 で線形結合\n",
    "    layer5: 10 次元の出力層\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.input_size = 784\n",
    "        self.output_size = 10\n",
    "        self.hidden_size_list = [100, 100, 100, 100]\n",
    "        self.all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
    "        self.hidden_layer_num = len(self.hidden_size_list)\n",
    "        self.weight_decay_lambda =0\n",
    "        self.params = {}\n",
    "        self.layers = {}\n",
    "        self.grads = {}\n",
    "        self.norms = {}\n",
    "        self.momentum = 0.9\n",
    "\n",
    "        # パラメータの初期化\n",
    "        for idx in range(1, len(self.all_size_list)):\n",
    "            # 線形結合層のパラメータ\n",
    "            self.params['w' + str(idx)] = np.random.randn(self.all_size_list[idx-1], self.all_size_list[idx]) * 0.085\n",
    "            self.params['b' + str(idx)] = np.zeros(self.all_size_list[idx], dtype=float)\n",
    "            \n",
    "            # バッチ正規化でシフトさせるときに用いるγとβを更新するパラメータとし初期化\n",
    "            # mu と sigma は実行時の平均と分散\n",
    "            if idx != len(self.all_size_list) - 1:\n",
    "                self.params['gamma' + str(idx)] = np.ones(self.all_size_list[idx])\n",
    "                self.params['beta' + str(idx)] = np.zeros(self.all_size_list[idx])\n",
    "                self.norms['mu' + str(idx)] = None\n",
    "                self.norms['var' + str(idx)] = None\n",
    "        \n",
    "    def forward(self, x, train_flg=False):\n",
    "        relu = lambda x : np.maximum(0, x)  # 活性化関数として ReLU を使用\n",
    "        self.layers['layer0'] = x\n",
    "        for idx in range(1, len(self.all_size_list) - 1):\n",
    "            # 線形結合層\n",
    "            w = self.params['w' + str(idx)]\n",
    "            b = self.params['b' + str(idx)]\n",
    "            x = self.layers['layer' + str(idx - 1)]\n",
    "            x = np.dot(x, w) + b\n",
    "            \n",
    "            # バッチ正規化\n",
    "            # 平均と分散の初期化\n",
    "            if self.norms['mu' + str(idx)] is None:\n",
    "                N, D = x.shape\n",
    "                self.norms['mu' + str(idx)] = np.zeros(D)\n",
    "                self.norms['var' + str(idx)] = np.zeros(D)\n",
    "            if train_flg:\n",
    "                mu = x.mean(axis=0)           # 今回のミニバッチの平均\n",
    "                xc = x - mu                   # 今回のミニバッチの平均との差分\n",
    "                var = np.mean(xc**2, axis=0)  # 今回のミニバッチの分散\n",
    "                std = np.sqrt(var + 10e-7)    # 今回のミニバッチの標準偏差\n",
    "                xn = xc / std                 # 正規化\n",
    "\n",
    "                # 全体の平均と分散を移動平均により求める\n",
    "                self.norms['mu' + str(idx)] = self.momentum * self.norms['mu' + str(idx)] + (1-self.momentum) * mu\n",
    "                self.norms['var' + str(idx)] = self.momentum * self.norms['var' + str(idx)] + (1-self.momentum) * var\n",
    "                \n",
    "                # 誤差逆伝播で使う中間データ\n",
    "                self.norms['xc' + str(idx)] = xc\n",
    "                self.norms['xn' + str(idx)] = xn\n",
    "                self.norms['std' + str(idx)] = std\n",
    "                self.norms['size' + str(idx)] = x.shape[0]\n",
    "            else:\n",
    "                # テスト時は全体の平均と分散を使って正規化\n",
    "                xc = x - self.norms['mu' + str(idx)]\n",
    "                xn = xc / ((np.sqrt(self.norms['var' + str(idx)] + 10e-7)))\n",
    "                \n",
    "            # バッチ正規化でシフト\n",
    "            shifted = self.params['gamma' + str(idx)] * xn + self.params['beta' + str(idx)]\n",
    "            \n",
    "            # relu を使って活性化\n",
    "            self.layers['layer' + str(idx)] = relu(shifted)\n",
    "\n",
    "        # 出力層\n",
    "        idx = len(self.all_size_list) - 1\n",
    "        w = self.params['w' + str(idx)]\n",
    "        b = self.params['b' + str(idx)]\n",
    "        x = self.layers['layer' + str(idx - 1)]\n",
    "        self.layers['layer' + str(idx)] = softmax(np.dot(x, w) + b)\n",
    "        \n",
    "        return self.layers['layer' + str(idx)]\n",
    "        \n",
    "\n",
    "    def loss(self, y, t):\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def backward(self, t, y):\n",
    "        # 出力層における誤差の勾配（クロスエントロピー関数の勾配）\n",
    "        delta = (y - t) / t.shape[0]\n",
    "        \n",
    "        # 出力層手前の線形結合層における勾配の逆伝播\n",
    "        self.grads['b5'] = np.sum(delta, axis=0)\n",
    "        self.grads['w5'] = np.dot(self.layers['layer4'].transpose(), delta)\n",
    "        \n",
    "        # 誤差逆伝播\n",
    "        for idx in range(4, 0, -1):\n",
    "            delta = np.dot(delta, self.params['w' + str(idx + 1)].transpose())\n",
    "            \n",
    "            # relu の微分\n",
    "            delta = delta *  (self.layers['layer' + str(idx)] > 0)\n",
    "            \n",
    "            # バッチ正規化における勾配の逆伝播\n",
    "            self.grads['beta' + str(idx)] = delta.sum(axis=0)\n",
    "            self.grads['gamma' + str(idx)] = np.sum(self.norms['xn' + str(idx)] * delta, axis=0)\n",
    "            dxn = self.params['gamma' + str(idx)] * delta\n",
    "            dxc = dxn / self.norms['std' + str(idx)]\n",
    "            dstd = -np.sum((dxn * self.norms['xc' + str(idx)]) / (self.norms['std' + str(idx)] * self.norms['std' + str(idx)]), axis=0)\n",
    "            dvar = 0.5 * dstd / self.norms['std' + str(idx)]\n",
    "            dxc += (2.0 / self.norms['size' + str(idx)]) * self.norms['xc' + str(idx)] * dvar\n",
    "            dmu = np.sum(dxc, axis=0)\n",
    "            delta = dxc - dmu / self.norms['size' + str(idx)]\n",
    "            \n",
    "            # 線形結合層における勾配の逆伝播\n",
    "            self.grads['b' + str(idx)] = np.sum(delta, axis=0)\n",
    "            self.grads['w' + str(idx)] = np.dot(self.layers['layer'+str(idx - 1)].transpose(), delta)\n",
    "            \n",
    "        return self.grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = mnistMultiLayerBatchNet()\n",
    "nobn = mnistMultiLayerNet()\n",
    "adambn = mnistMultiLayerBatchNet()\n",
    "adamnobn = mnistMultiLayerNet()\n",
    "\n",
    "bn_acc_list = []\n",
    "nobn_acc_list = []\n",
    "adambn_acc_list = []\n",
    "adamnobn_acc_list = []\n",
    "\n",
    "sgd = SGD(lr = 0.01)\n",
    "adam = Adam(lr=0.01)\n",
    "\n",
    "# ミニバッチアルゴリズム\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 | NoBatch ACCURACY (SGD) 12.39% | Batch ACCURACY (SGD)9.96% | NoBatch ACCURACY (Adam)20.86% | Batch ACCURACY (Adam) 17.82%\n",
      "EPOCH 2 | NoBatch ACCURACY (SGD) 17.20% | Batch ACCURACY (SGD)10.64% | NoBatch ACCURACY (Adam)23.00% | Batch ACCURACY (Adam) 24.63%\n",
      "EPOCH 3 | NoBatch ACCURACY (SGD) 19.42% | Batch ACCURACY (SGD)12.64% | NoBatch ACCURACY (Adam)31.49% | Batch ACCURACY (Adam) 36.39%\n",
      "EPOCH 4 | NoBatch ACCURACY (SGD) 21.86% | Batch ACCURACY (SGD)15.87% | NoBatch ACCURACY (Adam)34.79% | Batch ACCURACY (Adam) 40.78%\n",
      "EPOCH 5 | NoBatch ACCURACY (SGD) 23.38% | Batch ACCURACY (SGD)18.47% | NoBatch ACCURACY (Adam)37.81% | Batch ACCURACY (Adam) 42.71%\n",
      "EPOCH 6 | NoBatch ACCURACY (SGD) 24.59% | Batch ACCURACY (SGD)20.59% | NoBatch ACCURACY (Adam)37.82% | Batch ACCURACY (Adam) 44.34%\n",
      "EPOCH 7 | NoBatch ACCURACY (SGD) 25.91% | Batch ACCURACY (SGD)22.51% | NoBatch ACCURACY (Adam)40.66% | Batch ACCURACY (Adam) 45.69%\n",
      "EPOCH 8 | NoBatch ACCURACY (SGD) 26.87% | Batch ACCURACY (SGD)24.05% | NoBatch ACCURACY (Adam)40.25% | Batch ACCURACY (Adam) 45.28%\n",
      "EPOCH 9 | NoBatch ACCURACY (SGD) 27.56% | Batch ACCURACY (SGD)25.25% | NoBatch ACCURACY (Adam)40.71% | Batch ACCURACY (Adam) 45.71%\n",
      "EPOCH 10 | NoBatch ACCURACY (SGD) 28.12% | Batch ACCURACY (SGD)26.32% | NoBatch ACCURACY (Adam)40.13% | Batch ACCURACY (Adam) 45.66%\n",
      "EPOCH 11 | NoBatch ACCURACY (SGD) 28.62% | Batch ACCURACY (SGD)27.49% | NoBatch ACCURACY (Adam)41.91% | Batch ACCURACY (Adam) 45.43%\n",
      "EPOCH 12 | NoBatch ACCURACY (SGD) 29.23% | Batch ACCURACY (SGD)28.49% | NoBatch ACCURACY (Adam)39.99% | Batch ACCURACY (Adam) 44.64%\n",
      "EPOCH 13 | NoBatch ACCURACY (SGD) 29.54% | Batch ACCURACY (SGD)29.26% | NoBatch ACCURACY (Adam)38.21% | Batch ACCURACY (Adam) 45.97%\n",
      "EPOCH 14 | NoBatch ACCURACY (SGD) 29.84% | Batch ACCURACY (SGD)30.32% | NoBatch ACCURACY (Adam)41.72% | Batch ACCURACY (Adam) 45.89%\n",
      "EPOCH 15 | NoBatch ACCURACY (SGD) 30.19% | Batch ACCURACY (SGD)31.15% | NoBatch ACCURACY (Adam)41.11% | Batch ACCURACY (Adam) 45.05%\n",
      "EPOCH 16 | NoBatch ACCURACY (SGD) 30.46% | Batch ACCURACY (SGD)31.79% | NoBatch ACCURACY (Adam)42.26% | Batch ACCURACY (Adam) 45.27%\n",
      "EPOCH 17 | NoBatch ACCURACY (SGD) 30.69% | Batch ACCURACY (SGD)32.19% | NoBatch ACCURACY (Adam)40.89% | Batch ACCURACY (Adam) 45.26%\n",
      "EPOCH 18 | NoBatch ACCURACY (SGD) 31.07% | Batch ACCURACY (SGD)32.87% | NoBatch ACCURACY (Adam)43.91% | Batch ACCURACY (Adam) 45.65%\n",
      "EPOCH 19 | NoBatch ACCURACY (SGD) 31.40% | Batch ACCURACY (SGD)33.39% | NoBatch ACCURACY (Adam)42.76% | Batch ACCURACY (Adam) 45.76%\n",
      "EPOCH 20 | NoBatch ACCURACY (SGD) 31.64% | Batch ACCURACY (SGD)33.80% | NoBatch ACCURACY (Adam)42.34% | Batch ACCURACY (Adam) 45.22%\n"
     ]
    }
   ],
   "source": [
    "#データ数:640, 回転0\n",
    "#データ数がかなり少ないためランダムな分割の仕方は少ないため、20EPOCH分割しても最後の方はほとんど変化はないと思われる。\n",
    "for epoch in range(20):\n",
    "    # ランダムにミニバッチへ分割するために、インデックスをランダムに並び替える\n",
    "    perm = np.random.permutation(len(x_train2))\n",
    "    # batch_size ごとにデータを読み込んで学習させる\n",
    "    for idx in np.arange(0, len(perm), batch_size):\n",
    "        x = x_train2[perm[idx:idx+batch_size]] \n",
    "        t =  t_train2[perm[idx:idx+batch_size]]\n",
    "        \n",
    "        y = bn.forward(x, train_flg=True)\n",
    "        grads = bn.backward(t, y)\n",
    "        sgd.update(bn.params,grads)\n",
    "        \n",
    "        y = adambn.forward(x, train_flg=True)\n",
    "        grads = adambn.backward(t, y)\n",
    "        adam.update(adambn.params,grads)\n",
    "        \n",
    "        y = nobn.forward(x)\n",
    "        grads = nobn.backward(t,y)\n",
    "        sgd.update(nobn.params, grads)\n",
    "        \n",
    "        y = adamnobn.forward(x)\n",
    "        grads = adamnobn.backward(t, y)\n",
    "        adam.update(adamnobn.params,grads)\n",
    "\n",
    "    y_test = bn.forward(x_test)\n",
    "    bn_acc_list.append((y_test.argmax(axis=1) == t_test.argmax(axis=1)).mean())\n",
    "    y_test = nobn.forward(x_test)\n",
    "    nobn_acc_list.append((y_test.argmax(axis=1) == t_test.argmax(axis=1)).mean())\n",
    "    y_test = adambn.forward(x_test)\n",
    "    adambn_acc_list.append((y_test.argmax(axis=1) == t_test.argmax(axis=1)).mean())\n",
    "    y_test = adamnobn.forward(x_test)\n",
    "    adamnobn_acc_list.append((y_test.argmax(axis=1) == t_test.argmax(axis=1)).mean())\n",
    "\n",
    "    print(f'EPOCH {epoch + 1} | NoBatch ACCURACY (SGD) {nobn_acc_list[-1]:.2%} | Batch ACCURACY (SGD){bn_acc_list[-1]:.2%} | NoBatch ACCURACY (Adam){adamnobn_acc_list[-1]:.2%} | Batch ACCURACY (Adam) {adambn_acc_list[-1]:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 | NoBatch ACCURACY (SGD) 15.95% | Batch ACCURACY (SGD)11.59% | NoBatch ACCURACY (Adam)18.75% | Batch ACCURACY (Adam) 30.73%\n",
      "EPOCH 2 | NoBatch ACCURACY (SGD) 22.45% | Batch ACCURACY (SGD)18.86% | NoBatch ACCURACY (Adam)26.99% | Batch ACCURACY (Adam) 60.64%\n",
      "EPOCH 3 | NoBatch ACCURACY (SGD) 28.44% | Batch ACCURACY (SGD)23.25% | NoBatch ACCURACY (Adam)31.71% | Batch ACCURACY (Adam) 64.91%\n",
      "EPOCH 4 | NoBatch ACCURACY (SGD) 32.72% | Batch ACCURACY (SGD)27.62% | NoBatch ACCURACY (Adam)40.44% | Batch ACCURACY (Adam) 71.20%\n",
      "EPOCH 5 | NoBatch ACCURACY (SGD) 35.99% | Batch ACCURACY (SGD)31.45% | NoBatch ACCURACY (Adam)48.29% | Batch ACCURACY (Adam) 74.34%\n",
      "EPOCH 6 | NoBatch ACCURACY (SGD) 37.84% | Batch ACCURACY (SGD)34.90% | NoBatch ACCURACY (Adam)47.07% | Batch ACCURACY (Adam) 70.72%\n",
      "EPOCH 7 | NoBatch ACCURACY (SGD) 40.10% | Batch ACCURACY (SGD)37.69% | NoBatch ACCURACY (Adam)51.83% | Batch ACCURACY (Adam) 72.74%\n",
      "EPOCH 8 | NoBatch ACCURACY (SGD) 39.28% | Batch ACCURACY (SGD)40.03% | NoBatch ACCURACY (Adam)59.00% | Batch ACCURACY (Adam) 68.34%\n",
      "EPOCH 9 | NoBatch ACCURACY (SGD) 40.52% | Batch ACCURACY (SGD)42.34% | NoBatch ACCURACY (Adam)61.52% | Batch ACCURACY (Adam) 67.39%\n",
      "EPOCH 10 | NoBatch ACCURACY (SGD) 41.89% | Batch ACCURACY (SGD)44.52% | NoBatch ACCURACY (Adam)59.39% | Batch ACCURACY (Adam) 68.25%\n",
      "EPOCH 11 | NoBatch ACCURACY (SGD) 42.16% | Batch ACCURACY (SGD)46.68% | NoBatch ACCURACY (Adam)61.37% | Batch ACCURACY (Adam) 66.90%\n",
      "EPOCH 12 | NoBatch ACCURACY (SGD) 44.29% | Batch ACCURACY (SGD)48.55% | NoBatch ACCURACY (Adam)63.03% | Batch ACCURACY (Adam) 66.76%\n",
      "EPOCH 13 | NoBatch ACCURACY (SGD) 44.19% | Batch ACCURACY (SGD)49.91% | NoBatch ACCURACY (Adam)66.24% | Batch ACCURACY (Adam) 65.66%\n",
      "EPOCH 14 | NoBatch ACCURACY (SGD) 46.25% | Batch ACCURACY (SGD)51.64% | NoBatch ACCURACY (Adam)66.90% | Batch ACCURACY (Adam) 68.95%\n",
      "EPOCH 15 | NoBatch ACCURACY (SGD) 46.47% | Batch ACCURACY (SGD)52.97% | NoBatch ACCURACY (Adam)67.46% | Batch ACCURACY (Adam) 68.19%\n",
      "EPOCH 16 | NoBatch ACCURACY (SGD) 46.24% | Batch ACCURACY (SGD)53.99% | NoBatch ACCURACY (Adam)67.66% | Batch ACCURACY (Adam) 65.75%\n",
      "EPOCH 17 | NoBatch ACCURACY (SGD) 47.63% | Batch ACCURACY (SGD)55.30% | NoBatch ACCURACY (Adam)66.20% | Batch ACCURACY (Adam) 68.06%\n",
      "EPOCH 18 | NoBatch ACCURACY (SGD) 46.69% | Batch ACCURACY (SGD)56.26% | NoBatch ACCURACY (Adam)65.79% | Batch ACCURACY (Adam) 65.81%\n",
      "EPOCH 19 | NoBatch ACCURACY (SGD) 47.09% | Batch ACCURACY (SGD)57.47% | NoBatch ACCURACY (Adam)66.58% | Batch ACCURACY (Adam) 67.16%\n",
      "EPOCH 20 | NoBatch ACCURACY (SGD) 49.29% | Batch ACCURACY (SGD)58.15% | NoBatch ACCURACY (Adam)68.94% | Batch ACCURACY (Adam) 69.76%\n"
     ]
    }
   ],
   "source": [
    "#データ数:1920, 回転0,15, 345\n",
    "#データ数640しか使っていないが、回転させてデータを増やしたことにより精度が全体的に1.5倍程度上昇。\n",
    "#データの種類は少ないため、epoch数が多いと過学習しやすいのではないか。\n",
    "if os.path.exists('mnist_784'):\n",
    "    with open('mnist_784','rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "else:\n",
    "    mnist = datasets.fetch_openml('mnist_784')\n",
    "    with open('mnist_784', 'wb') as f:\n",
    "        pickle.dump(mnist, f)\n",
    "# 訓練データ、テストデータを取ってくる\n",
    "X, T = mnist.data, mnist.target\n",
    "# 訓練データとテストデータに分ける\n",
    "x_train, x_test, t_train, t_test = train_test_split(X, T, test_size=0.2)\n",
    "# データの形を確認\n",
    "#print(x_train.shape)\n",
    "#print(t_train.shape)\n",
    "# 640個だけランダムにデータを抽出\n",
    "x_train2 = np.zeros((640,784))\n",
    "t_train2 = np.zeros(640)\n",
    "perm = np.random.permutation(len(x_train))\n",
    "for idx in range(640):\n",
    "    x_train2[idx] = x_train[perm[idx]]\n",
    "    t_train2[idx] = t_train[perm[idx]]\n",
    "# int型のone-hot-vectorに変換\n",
    "t_train = np.eye(10)[t_train.astype(\"int\")]\n",
    "t_train2 = np.eye(10)[t_train2.astype(\"int\")]\n",
    "t_test = np.eye(10)[t_test.astype(\"int\")]\n",
    "\n",
    "bn = mnistMultiLayerBatchNet()\n",
    "nobn = mnistMultiLayerNet()\n",
    "adambn = mnistMultiLayerBatchNet()\n",
    "adamnobn = mnistMultiLayerNet()\n",
    "\n",
    "bn_acc_list = []\n",
    "nobn_acc_list = []\n",
    "adambn_acc_list = []\n",
    "adamnobn_acc_list = []\n",
    "\n",
    "sgd = SGD(lr = 0.01)\n",
    "adam = Adam(lr=0.01)\n",
    "\n",
    "# ミニバッチアルゴリズム\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "x_train0_15_345 = np.zeros((1920, 784))\n",
    "t_train0_15_345 = np.zeros((1920, 10))\n",
    "x_train0_15_345[0:640, :] = x_train2\n",
    "x_train0_15_345[640:1280, :] = x_train15\n",
    "x_train0_15_345[1280:, :] = x_train345\n",
    "t_train0_15_345[0:640, :] = t_train2\n",
    "t_train0_15_345[640:1280, :] = t_train2\n",
    "t_train0_15_345[1280:, :] = t_train2\n",
    "for epoch in range(20):\n",
    "    # ランダムにミニバッチへ分割するために、インデックスをランダムに並び替える\n",
    "    perm = np.random.permutation(len(x_train0_15_345))\n",
    "    # batch_size ごとにデータを読み込んで学習させる\n",
    "    for idx in np.arange(0, len(perm), batch_size):\n",
    "        x = x_train0_15_345[perm[idx:idx+batch_size]] \n",
    "        t =  t_train0_15_345[perm[idx:idx+batch_size]]\n",
    "        \n",
    "        y = bn.forward(x, train_flg=True)\n",
    "        grads = bn.backward(t, y)\n",
    "        sgd.update(bn.params,grads)\n",
    "        \n",
    "        y = adambn.forward(x, train_flg=True)\n",
    "        grads = adambn.backward(t, y)\n",
    "        adam.update(adambn.params,grads)\n",
    "        \n",
    "        y = nobn.forward(x)\n",
    "        grads = nobn.backward(t,y)\n",
    "        sgd.update(nobn.params, grads)\n",
    "        \n",
    "        y = adamnobn.forward(x)\n",
    "        grads = adamnobn.backward(t, y)\n",
    "        adam.update(adamnobn.params,grads)\n",
    "\n",
    "    y_test = bn.forward(x_test)\n",
    "    bn_acc_list.append((y_test.argmax(axis=1) == t_test.argmax(axis=1)).mean())\n",
    "    y_test = nobn.forward(x_test)\n",
    "    nobn_acc_list.append((y_test.argmax(axis=1) == t_test.argmax(axis=1)).mean())\n",
    "    y_test = adambn.forward(x_test)\n",
    "    adambn_acc_list.append((y_test.argmax(axis=1) == t_test.argmax(axis=1)).mean())\n",
    "    y_test = adamnobn.forward(x_test)\n",
    "    adamnobn_acc_list.append((y_test.argmax(axis=1) == t_test.argmax(axis=1)).mean())\n",
    "\n",
    "    print(f'EPOCH {epoch + 1} | NoBatch ACCURACY (SGD) {nobn_acc_list[-1]:.2%} | Batch ACCURACY (SGD){bn_acc_list[-1]:.2%} | NoBatch ACCURACY (Adam){adamnobn_acc_list[-1]:.2%} | Batch ACCURACY (Adam) {adambn_acc_list[-1]:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 | NoBatch ACCURACY (SGD) 41.24% | Batch ACCURACY (SGD)17.42% | NoBatch ACCURACY (Adam)49.29% | Batch ACCURACY (Adam) 55.59%\n",
      "EPOCH 2 | NoBatch ACCURACY (SGD) 52.66% | Batch ACCURACY (SGD)32.94% | NoBatch ACCURACY (Adam)69.38% | Batch ACCURACY (Adam) 81.54%\n",
      "EPOCH 3 | NoBatch ACCURACY (SGD) 60.12% | Batch ACCURACY (SGD)42.26% | NoBatch ACCURACY (Adam)74.79% | Batch ACCURACY (Adam) 85.61%\n",
      "EPOCH 4 | NoBatch ACCURACY (SGD) 63.64% | Batch ACCURACY (SGD)49.50% | NoBatch ACCURACY (Adam)82.09% | Batch ACCURACY (Adam) 87.14%\n",
      "EPOCH 5 | NoBatch ACCURACY (SGD) 65.07% | Batch ACCURACY (SGD)54.57% | NoBatch ACCURACY (Adam)81.29% | Batch ACCURACY (Adam) 87.12%\n",
      "EPOCH 6 | NoBatch ACCURACY (SGD) 67.19% | Batch ACCURACY (SGD)58.99% | NoBatch ACCURACY (Adam)84.29% | Batch ACCURACY (Adam) 87.68%\n",
      "EPOCH 7 | NoBatch ACCURACY (SGD) 70.16% | Batch ACCURACY (SGD)62.59% | NoBatch ACCURACY (Adam)85.90% | Batch ACCURACY (Adam) 87.57%\n",
      "EPOCH 8 | NoBatch ACCURACY (SGD) 69.78% | Batch ACCURACY (SGD)65.69% | NoBatch ACCURACY (Adam)84.07% | Batch ACCURACY (Adam) 88.40%\n",
      "EPOCH 9 | NoBatch ACCURACY (SGD) 71.46% | Batch ACCURACY (SGD)68.41% | NoBatch ACCURACY (Adam)85.14% | Batch ACCURACY (Adam) 88.69%\n",
      "EPOCH 10 | NoBatch ACCURACY (SGD) 70.56% | Batch ACCURACY (SGD)70.19% | NoBatch ACCURACY (Adam)87.58% | Batch ACCURACY (Adam) 88.74%\n",
      "EPOCH 11 | NoBatch ACCURACY (SGD) 74.82% | Batch ACCURACY (SGD)72.16% | NoBatch ACCURACY (Adam)86.16% | Batch ACCURACY (Adam) 88.17%\n",
      "EPOCH 12 | NoBatch ACCURACY (SGD) 75.50% | Batch ACCURACY (SGD)73.96% | NoBatch ACCURACY (Adam)86.35% | Batch ACCURACY (Adam) 88.95%\n",
      "EPOCH 13 | NoBatch ACCURACY (SGD) 76.05% | Batch ACCURACY (SGD)75.41% | NoBatch ACCURACY (Adam)87.67% | Batch ACCURACY (Adam) 89.36%\n",
      "EPOCH 14 | NoBatch ACCURACY (SGD) 77.16% | Batch ACCURACY (SGD)76.57% | NoBatch ACCURACY (Adam)86.87% | Batch ACCURACY (Adam) 88.90%\n",
      "EPOCH 15 | NoBatch ACCURACY (SGD) 77.25% | Batch ACCURACY (SGD)77.64% | NoBatch ACCURACY (Adam)84.20% | Batch ACCURACY (Adam) 89.07%\n",
      "EPOCH 16 | NoBatch ACCURACY (SGD) 77.11% | Batch ACCURACY (SGD)78.57% | NoBatch ACCURACY (Adam)86.30% | Batch ACCURACY (Adam) 89.16%\n",
      "EPOCH 17 | NoBatch ACCURACY (SGD) 78.24% | Batch ACCURACY (SGD)79.43% | NoBatch ACCURACY (Adam)84.84% | Batch ACCURACY (Adam) 85.79%\n",
      "EPOCH 18 | NoBatch ACCURACY (SGD) 77.94% | Batch ACCURACY (SGD)80.23% | NoBatch ACCURACY (Adam)84.29% | Batch ACCURACY (Adam) 86.09%\n",
      "EPOCH 19 | NoBatch ACCURACY (SGD) 78.54% | Batch ACCURACY (SGD)80.72% | NoBatch ACCURACY (Adam)84.56% | Batch ACCURACY (Adam) 87.57%\n",
      "EPOCH 20 | NoBatch ACCURACY (SGD) 78.79% | Batch ACCURACY (SGD)81.16% | NoBatch ACCURACY (Adam)87.04% | Batch ACCURACY (Adam) 88.61%\n"
     ]
    }
   ],
   "source": [
    "# データ数:1920, 回転0\n",
    "# データ数は同じなのに別のデータを入れている分、角度を変えているとはいえ似たようなデータが多く含まれる場合より汎化性能が高いため\n",
    "#640個のデータを3倍に増やしたものより精度がいいと思われる。だが、56000よりはデータ数はとても少ないので、\n",
    "#学習の進みがより早いAdamでは過学習が進みEPOCH17以降では特に過学習による精度の低下が見られる。\n",
    "if os.path.exists('mnist_784'):\n",
    "    with open('mnist_784','rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "else:\n",
    "    mnist = datasets.fetch_openml('mnist_784')\n",
    "    with open('mnist_784', 'wb') as f:\n",
    "        pickle.dump(mnist, f)\n",
    "# 訓練データ、テストデータを取ってくる\n",
    "X, T = mnist.data, mnist.target\n",
    "# 訓練データとテストデータに分ける\n",
    "x_train, x_test, t_train, t_test = train_test_split(X, T, test_size=0.2)\n",
    "# データの形を確認\n",
    "#print(x_train.shape)\n",
    "#print(t_train.shape)\n",
    "# 640個だけランダムにデータを抽出\n",
    "x_train2 = np.zeros((1920,784))\n",
    "t_train2 = np.zeros(1920)\n",
    "perm = np.random.permutation(len(x_train))\n",
    "for idx in range(1920):\n",
    "    x_train2[idx] = x_train[perm[idx]]\n",
    "    t_train2[idx] = t_train[perm[idx]]\n",
    "# int型のone-hot-vectorに変換\n",
    "t_train = np.eye(10)[t_train.astype(\"int\")]\n",
    "t_train2 = np.eye(10)[t_train2.astype(\"int\")]\n",
    "t_test = np.eye(10)[t_test.astype(\"int\")]\n",
    "\n",
    "bn = mnistMultiLayerBatchNet()\n",
    "nobn = mnistMultiLayerNet()\n",
    "adambn = mnistMultiLayerBatchNet()\n",
    "adamnobn = mnistMultiLayerNet()\n",
    "\n",
    "bn_acc_list = []\n",
    "nobn_acc_list = []\n",
    "adambn_acc_list = []\n",
    "adamnobn_acc_list = []\n",
    "\n",
    "sgd = SGD(lr = 0.01)\n",
    "adam = Adam(lr=0.01)\n",
    "\n",
    "# ミニバッチアルゴリズム\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(20):\n",
    "    # ランダムにミニバッチへ分割するために、インデックスをランダムに並び替える\n",
    "    perm = np.random.permutation(len(x_train2))\n",
    "    # batch_size ごとにデータを読み込んで学習させる\n",
    "    for idx in np.arange(0, len(perm), batch_size):\n",
    "        x = x_train2[perm[idx:idx+batch_size]] \n",
    "        t =  t_train2[perm[idx:idx+batch_size]]\n",
    "        \n",
    "        y = bn.forward(x, train_flg=True)\n",
    "        grads = bn.backward(t, y)\n",
    "        sgd.update(bn.params,grads)\n",
    "        \n",
    "        y = adambn.forward(x, train_flg=True)\n",
    "        grads = adambn.backward(t, y)\n",
    "        adam.update(adambn.params,grads)\n",
    "        \n",
    "        y = nobn.forward(x)\n",
    "        grads = nobn.backward(t,y)\n",
    "        sgd.update(nobn.params, grads)\n",
    "        \n",
    "        y = adamnobn.forward(x)\n",
    "        grads = adamnobn.backward(t, y)\n",
    "        adam.update(adamnobn.params,grads)\n",
    "\n",
    "    y_test = bn.forward(x_test)\n",
    "    bn_acc_list.append((y_test.argmax(axis=1) == t_test.argmax(axis=1)).mean())\n",
    "    y_test = nobn.forward(x_test)\n",
    "    nobn_acc_list.append((y_test.argmax(axis=1) == t_test.argmax(axis=1)).mean())\n",
    "    y_test = adambn.forward(x_test)\n",
    "    adambn_acc_list.append((y_test.argmax(axis=1) == t_test.argmax(axis=1)).mean())\n",
    "    y_test = adamnobn.forward(x_test)\n",
    "    adamnobn_acc_list.append((y_test.argmax(axis=1) == t_test.argmax(axis=1)).mean())\n",
    "\n",
    "    print(f'EPOCH {epoch + 1} | NoBatch ACCURACY (SGD) {nobn_acc_list[-1]:.2%} | Batch ACCURACY (SGD){bn_acc_list[-1]:.2%} | NoBatch ACCURACY (Adam){adamnobn_acc_list[-1]:.2%} | Batch ACCURACY (Adam) {adambn_acc_list[-1]:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 | NoBatch ACCURACY (SGD) 15.70% | Batch ACCURACY (SGD)14.63% | NoBatch ACCURACY (Adam)22.61% | Batch ACCURACY (Adam) 42.72%\n",
      "EPOCH 2 | NoBatch ACCURACY (SGD) 20.06% | Batch ACCURACY (SGD)21.10% | NoBatch ACCURACY (Adam)35.54% | Batch ACCURACY (Adam) 61.51%\n",
      "EPOCH 3 | NoBatch ACCURACY (SGD) 22.23% | Batch ACCURACY (SGD)28.04% | NoBatch ACCURACY (Adam)41.84% | Batch ACCURACY (Adam) 65.75%\n",
      "EPOCH 4 | NoBatch ACCURACY (SGD) 23.49% | Batch ACCURACY (SGD)33.73% | NoBatch ACCURACY (Adam)48.17% | Batch ACCURACY (Adam) 66.35%\n",
      "EPOCH 5 | NoBatch ACCURACY (SGD) 24.41% | Batch ACCURACY (SGD)37.96% | NoBatch ACCURACY (Adam)53.01% | Batch ACCURACY (Adam) 69.79%\n",
      "EPOCH 6 | NoBatch ACCURACY (SGD) 26.82% | Batch ACCURACY (SGD)41.20% | NoBatch ACCURACY (Adam)54.56% | Batch ACCURACY (Adam) 66.53%\n",
      "EPOCH 7 | NoBatch ACCURACY (SGD) 27.27% | Batch ACCURACY (SGD)43.74% | NoBatch ACCURACY (Adam)53.81% | Batch ACCURACY (Adam) 65.66%\n",
      "EPOCH 8 | NoBatch ACCURACY (SGD) 29.06% | Batch ACCURACY (SGD)45.99% | NoBatch ACCURACY (Adam)54.87% | Batch ACCURACY (Adam) 67.11%\n",
      "EPOCH 9 | NoBatch ACCURACY (SGD) 31.44% | Batch ACCURACY (SGD)47.94% | NoBatch ACCURACY (Adam)60.83% | Batch ACCURACY (Adam) 63.89%\n",
      "EPOCH 10 | NoBatch ACCURACY (SGD) 32.59% | Batch ACCURACY (SGD)49.71% | NoBatch ACCURACY (Adam)60.60% | Batch ACCURACY (Adam) 68.79%\n",
      "EPOCH 11 | NoBatch ACCURACY (SGD) 33.26% | Batch ACCURACY (SGD)51.62% | NoBatch ACCURACY (Adam)61.56% | Batch ACCURACY (Adam) 65.31%\n",
      "EPOCH 12 | NoBatch ACCURACY (SGD) 34.96% | Batch ACCURACY (SGD)52.82% | NoBatch ACCURACY (Adam)61.74% | Batch ACCURACY (Adam) 66.69%\n",
      "EPOCH 13 | NoBatch ACCURACY (SGD) 35.62% | Batch ACCURACY (SGD)54.09% | NoBatch ACCURACY (Adam)63.36% | Batch ACCURACY (Adam) 67.84%\n",
      "EPOCH 14 | NoBatch ACCURACY (SGD) 37.47% | Batch ACCURACY (SGD)55.35% | NoBatch ACCURACY (Adam)68.34% | Batch ACCURACY (Adam) 67.66%\n",
      "EPOCH 15 | NoBatch ACCURACY (SGD) 38.91% | Batch ACCURACY (SGD)56.24% | NoBatch ACCURACY (Adam)59.18% | Batch ACCURACY (Adam) 66.54%\n",
      "EPOCH 16 | NoBatch ACCURACY (SGD) 38.71% | Batch ACCURACY (SGD)57.38% | NoBatch ACCURACY (Adam)61.19% | Batch ACCURACY (Adam) 68.55%\n",
      "EPOCH 17 | NoBatch ACCURACY (SGD) 39.16% | Batch ACCURACY (SGD)58.37% | NoBatch ACCURACY (Adam)65.10% | Batch ACCURACY (Adam) 64.72%\n",
      "EPOCH 18 | NoBatch ACCURACY (SGD) 40.76% | Batch ACCURACY (SGD)58.71% | NoBatch ACCURACY (Adam)61.36% | Batch ACCURACY (Adam) 64.87%\n",
      "EPOCH 19 | NoBatch ACCURACY (SGD) 41.19% | Batch ACCURACY (SGD)59.51% | NoBatch ACCURACY (Adam)62.76% | Batch ACCURACY (Adam) 67.60%\n",
      "EPOCH 20 | NoBatch ACCURACY (SGD) 40.91% | Batch ACCURACY (SGD)60.04% | NoBatch ACCURACY (Adam)61.74% | Batch ACCURACY (Adam) 65.24%\n"
     ]
    }
   ],
   "source": [
    "#データ数:1920, 回転0, 10, 15, 345, 350\n",
    "# 0, 15, 345°の時より精度が落ちている。訓練データを加工したもののみを使っているので、過学習してしまったのかもしれない。\n",
    "if os.path.exists('mnist_784'):\n",
    "    with open('mnist_784','rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "else:\n",
    "    mnist = datasets.fetch_openml('mnist_784')\n",
    "    with open('mnist_784', 'wb') as f:\n",
    "        pickle.dump(mnist, f)\n",
    "# 訓練データ、テストデータを取ってくる\n",
    "X, T = mnist.data, mnist.target\n",
    "# 訓練データとテストデータに分ける\n",
    "x_train, x_test, t_train, t_test = train_test_split(X, T, test_size=0.2)\n",
    "# データの形を確認\n",
    "#print(x_train.shape)\n",
    "#print(t_train.shape)\n",
    "# 640個だけランダムにデータを抽出\n",
    "x_train2 = np.zeros((640,784))\n",
    "t_train2 = np.zeros(640)\n",
    "perm = np.random.permutation(len(x_train))\n",
    "for idx in range(640):\n",
    "    x_train2[idx] = x_train[perm[idx]]\n",
    "    t_train2[idx] = t_train[perm[idx]]\n",
    "# int型のone-hot-vectorに変換\n",
    "t_train = np.eye(10)[t_train.astype(\"int\")]\n",
    "t_train2 = np.eye(10)[t_train2.astype(\"int\")]\n",
    "t_test = np.eye(10)[t_test.astype(\"int\")]\n",
    "\n",
    "bn = mnistMultiLayerBatchNet()\n",
    "nobn = mnistMultiLayerNet()\n",
    "adambn = mnistMultiLayerBatchNet()\n",
    "adamnobn = mnistMultiLayerNet()\n",
    "\n",
    "bn_acc_list = []\n",
    "nobn_acc_list = []\n",
    "adambn_acc_list = []\n",
    "adamnobn_acc_list = []\n",
    "\n",
    "sgd = SGD(lr = 0.01)\n",
    "adam = Adam(lr=0.01)\n",
    "\n",
    "# ミニバッチアルゴリズム\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "x_train0_10_15_345_350 = np.zeros((3200, 784))\n",
    "t_train0_10_15_345_350 = np.zeros((3200, 10))\n",
    "x_train0_10_15_345_350[0:640, :] = x_train2\n",
    "x_train0_10_15_345_350[640:1280, :] = x_train10\n",
    "x_train0_10_15_345_350[1280:1920, :] = x_train15\n",
    "x_train0_10_15_345_350[1920:2560, :] = x_train345\n",
    "x_train0_10_15_345_350[2560:, :] = x_train350\n",
    "t_train0_10_15_345_350[0:640, :] = t_train2\n",
    "t_train0_10_15_345_350[640:1280, :] = t_train2\n",
    "t_train0_10_15_345_350[1280:1920, :] = t_train2\n",
    "t_train0_10_15_345_350[1920:2560, :] = t_train2\n",
    "t_train0_10_15_345_350[2560:, :] = t_train2\n",
    "\n",
    "for epoch in range(20):\n",
    "    # ランダムにミニバッチへ分割するために、インデックスをランダムに並び替える\n",
    "    perm = np.random.permutation(len(x_train0_10_15_345_350))\n",
    "    # batch_size ごとにデータを読み込んで学習させる\n",
    "    for idx in np.arange(0, len(perm), batch_size):\n",
    "        x = x_train0_10_15_345_350[perm[idx:idx+batch_size]] \n",
    "        t =  t_train0_10_15_345_350[perm[idx:idx+batch_size]]\n",
    "        \n",
    "        y = bn.forward(x, train_flg=True)\n",
    "        grads = bn.backward(t, y)\n",
    "        sgd.update(bn.params,grads)\n",
    "        \n",
    "        y = adambn.forward(x, train_flg=True)\n",
    "        grads = adambn.backward(t, y)\n",
    "        adam.update(adambn.params,grads)\n",
    "        \n",
    "        y = nobn.forward(x)\n",
    "        grads = nobn.backward(t,y)\n",
    "        sgd.update(nobn.params, grads)\n",
    "        \n",
    "        y = adamnobn.forward(x)\n",
    "        grads = adamnobn.backward(t, y)\n",
    "        adam.update(adamnobn.params,grads)\n",
    "\n",
    "    y_test = bn.forward(x_test)\n",
    "    bn_acc_list.append((y_test.argmax(axis=1) == t_test.argmax(axis=1)).mean())\n",
    "    y_test = nobn.forward(x_test)\n",
    "    nobn_acc_list.append((y_test.argmax(axis=1) == t_test.argmax(axis=1)).mean())\n",
    "    y_test = adambn.forward(x_test)\n",
    "    adambn_acc_list.append((y_test.argmax(axis=1) == t_test.argmax(axis=1)).mean())\n",
    "    y_test = adamnobn.forward(x_test)\n",
    "    adamnobn_acc_list.append((y_test.argmax(axis=1) == t_test.argmax(axis=1)).mean())\n",
    "\n",
    "    print(f'EPOCH {epoch + 1} | NoBatch ACCURACY (SGD) {nobn_acc_list[-1]:.2%} | Batch ACCURACY (SGD){bn_acc_list[-1]:.2%} | NoBatch ACCURACY (Adam){adamnobn_acc_list[-1]:.2%} | Batch ACCURACY (Adam) {adambn_acc_list[-1]:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
